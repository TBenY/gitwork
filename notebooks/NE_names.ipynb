{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "def extract_entities(text):\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'node'):\n",
    "                print chunk.node, ' '.join(c[0] for c in chunk.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.nltk.org/_modules/nltk/tree.html\n",
    "# http://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = nltk.sent_tokenize('tal is working at SF')\n",
    "# nltk.ne_chunk( o, binary = False)\n",
    "# nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(o[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tal', 'JJ'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('SF', 'NNP')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'tal is working at SF'\n",
    "nltk.sent_tokenize(sentence)\n",
    "sent1 = nltk.word_tokenize(text )\n",
    "sent2 = nltk.pos_tag(sent1)\n",
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('Jhon', 'NNP'),\n",
       " ('from', 'IN'),\n",
       " ('America', 'NNP')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am Jhon from America\"\n",
    "sent1 = nltk.word_tokenize(sentence )\n",
    "sent2 = nltk.pos_tag(sent1)\n",
    "\n",
    "sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Tree.subtrees of Tree('S', [('tal', 'JJ'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), Tree('NE', [('SF', 'NNP')])])>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent3 =  nltk.ne_chunk(sent2, binary=True)\n",
    "sent3.subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n",
      "('tal', 'JJ')\n",
      "<type 'tuple'>\n",
      "3\n",
      "[('tal', 'JJ'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('SF', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "t = sent3\n",
    "print(t.label())         # tree's constituent type\n",
    "print(t[0])             # tree's first child\n",
    "print(type(t[-1]) )            # tree's second child\n",
    "print(t.height())\n",
    "print(t.leaves())\n",
    "\n",
    "# print(t[1,1])\n",
    "# print(t[1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'tuple'>\n",
      "<type 'tuple'>\n",
      "<type 'tuple'>\n",
      "<type 'tuple'>\n",
      "<class 'nltk.tree.Tree'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Tree.subtrees of Tree('NE', [('SF', 'NNP')])>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub = sent3.subtrees\n",
    "# [x for x in sent3.label()]\n",
    "for x in sent3:\n",
    "    print(type(x))\n",
    "x.subtrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/tbenyakar/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAST, FIRST\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nameparser.parser import HumanName\n",
    "from pprint import pprint\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "    person_list = []\n",
    "    person = []\n",
    "    name = \"\"\n",
    "#     for subtree in sentt.subtrees(filter=lambda t: t.node == 'PERSON'):\n",
    "#         for leaf in subtree.leaves():\n",
    "#             person.append(leaf[0])\n",
    "#         if len(person) > 1: #avoid grabbing lone surnames\n",
    "#             for part in person:\n",
    "#                 name += part + ' '\n",
    "#             if name[:-1] not in person_list:\n",
    "#                 person_list.append(name[:-1])\n",
    "#             name = ''\n",
    "#         person = []\n",
    "\n",
    "    return sentt #(person_list)\n",
    "\n",
    "text = \"Some economists have responded positively to Bitcoin, including Francois R. Velde, senior economist of the Federal Reserve in Chicago who described it as \"\n",
    "# an elegant solution to the problem of creating a digital currency.\" In November 2013 Richard Branson announced that \n",
    "# Virgin Galactic would accept Bitcoin as payment, saying that he had invested \n",
    "# in Bitcoin and found it \"fascinating how a whole new global currency \n",
    "# has been created\", encouraging others to also invest in Bitcoin.\n",
    "# Other economists commenting on Bitcoin have been critical. \n",
    "# Economist Paul Krugman has suggested that the structure of the currency \n",
    "# incentivizes hoarding and that its value derives from the expectation that \n",
    "# others will accept it as payment. Economist Larry Summers has expressed \n",
    "# a \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \n",
    "# strategist for ConvergEx Group, has remarked on the effect of increasing \n",
    "# use of Bitcoin and its restricted supply, noting, \"When incremental \n",
    "# adoption meets relatively fixed supply, it should be no surprise that \n",
    "# prices go up. And thatâ€™s exactly what is happening to BTC prices. \"\n",
    "\n",
    "names = get_human_names(text)\n",
    "print \"LAST, FIRST\"\n",
    "# for name in names: \n",
    "#     last_first = HumanName(name).last + ', ' + HumanName(name).first\n",
    "#     print last_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for l in names.subtrees(filter=lambda t: t[0][1] == 'PERSON'):\n",
    "    print(l.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for subtree in names.subtrees(filter=lambda t: t.label == 'PERSON'):\n",
    "    print(subtree())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Some', 'DT')\n",
      "('economists', 'NNS')\n",
      "('have', 'VBP')\n",
      "('responded', 'VBN')\n",
      "('positively', 'RB')\n",
      "('to', 'TO')\n",
      "(GPE Bitcoin/NNP)\n",
      "(',', ',')\n",
      "('including', 'VBG')\n",
      "(PERSON Francois/NNP R./NNP Velde/NNP)\n",
      "(',', ',')\n",
      "('senior', 'JJ')\n",
      "('economist', 'NN')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "(ORGANIZATION Federal/NNP Reserve/NNP)\n",
      "('in', 'IN')\n",
      "(GPE Chicago/NNP)\n",
      "('who', 'WP')\n",
      "('described', 'VBN')\n",
      "('it', 'PRP')\n",
      "('as', 'IN')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IN'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for n in names:\n",
    "    print(n)\n",
    "n[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at stanford-ner/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-1f9b7385e6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanford-ner/all.3class.distsim.crf.ser.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stanford-ner/stanford-ner.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"YOUR TEXT GOES HERE\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tag/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_model, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         self._stanford_model = find_file(path_to_model,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    644\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    645\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 646\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdoutdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 579\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at stanford-ner/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag.stanford import NERTagger\n",
    "st = NERTagger('stanford-ner/all.3class.distsim.crf.ser.gz', 'stanford-ner/stanford-ner.jar')\n",
    "text = \"\"\"YOUR TEXT GOES HERE\"\"\"\n",
    "\n",
    "for sent in nltk.sent_tokenize(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(sent)\n",
    "    tags = st.tag(tokens)\n",
    "    for tag in tags:\n",
    "        if tag[1]=='PERSON': print tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
